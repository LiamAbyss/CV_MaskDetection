{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "\n",
    "Run this notebook if you want to create your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the folder (Masked, Unmasked, Half_Masked)\n",
    "name = \"Masked\"\n",
    "\n",
    "# Number of pictures to take\n",
    "datasetSize = 25\n",
    "\n",
    "if not os.path.exists('Output/faces/' + name):\n",
    "    os.mkdir('Output/faces/' + name)\n",
    "\n",
    "# Initialize mediapipe\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "stop = False\n",
    "count = len(os.listdir('Output/faces/' + name))\n",
    "datasetSize += count\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    \n",
    "    i = 0\n",
    "    while(not stop):\n",
    "        # Capture the video frame\n",
    "        # by frame\n",
    "        ret, img = vid.read()\n",
    "    \n",
    "        img.flags.writeable = False\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        cv2.flip(img, 1, img)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = face_detection.process(img)\n",
    "\n",
    "        img.flags.writeable = True\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        bounded_img = img.copy()\n",
    "\n",
    "        try:\n",
    "            if results.detections:\n",
    "                # Draw the face detection annotations on the image.\n",
    "                for detection in results.detections:\n",
    "                    \n",
    "                    # Get the bounding box of the face\n",
    "                    # and its characteristics\n",
    "                    bb = detection.location_data.relative_bounding_box\n",
    "                    x = bb.xmin\n",
    "                    y = bb.ymin\n",
    "                    h = bb.height\n",
    "                    w = bb.width\n",
    "\n",
    "                    camWidth = img.shape[1]\n",
    "                    camHeight = img.shape[0]\n",
    "\n",
    "                    # Convert x, y, h and w from percentages to pixels\n",
    "                    x = int(x * camWidth)\n",
    "                    y = int(y * camHeight)\n",
    "                    h = int(h * camHeight)\n",
    "                    w = int(w * camWidth)\n",
    "                    \n",
    "                    # Allows some time to move between the captures\n",
    "                    # Change the modulo value if you want more or less time between captures\n",
    "                    i = (i + 1) % 35\n",
    "                    if i == 0:\n",
    "                        file_name = \"face_\" + str(count) + \".jpg\"\n",
    "                        \n",
    "                        # Resize the image to 250x250px\n",
    "                        f = cv2.resize(img[y:y+h, x:x+w], (250, 250), interpolation = cv2.INTER_AREA)\n",
    "                        \n",
    "                        # Save the capture\n",
    "                        cv2.imwrite('./Output/faces/' + name + '/' + file_name, f)\n",
    "                        count += 1\n",
    "                        \n",
    "                        # Stop if we reached the wanted dataset size\n",
    "                        if count >= datasetSize:\n",
    "                            stop = True\n",
    "                    \n",
    "                    # Draw a rectangle to represent the bounding box\n",
    "                    cv2.rectangle(bounded_img, (x, y),(x + w, y + h), (255, 0, 0), 2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Increase output size for ergonomy\n",
    "        bounded_img = cv2.resize(bounded_img, (0,0), fx=1.5, fy=1.5)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', bounded_img)\n",
    "        \n",
    "        # Show the remaining frames to capture before the end\n",
    "        cv2.setWindowTitle('frame', 'frame : ' + str(abs(count - datasetSize)))\n",
    "        \n",
    "        # The 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
