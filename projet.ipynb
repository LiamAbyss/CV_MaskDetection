{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the folder (Masked, Unmasked, Half_Masked)\n",
    "name = \"Masked\"\n",
    "\n",
    "# Number of pictures to take\n",
    "datasetSize = 80\n",
    "\n",
    "if not os.path.exists('Output/faces/' + name):\n",
    "    os.mkdir('Output/faces/' + name)\n",
    "\n",
    "# Initialize mediapipe\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "stop = False\n",
    "count = len(os.listdir('Output/faces/' + name))\n",
    "datasetSize += count\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    \n",
    "    i = 0\n",
    "    while(not stop):\n",
    "        # Capture the video frame\n",
    "        # by frame\n",
    "        ret, img = vid.read()\n",
    "    \n",
    "        img.flags.writeable = False\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        cv2.flip(img, 1, img)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = face_detection.process(img)\n",
    "\n",
    "        img.flags.writeable = True\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        bounded_img = img.copy()\n",
    "\n",
    "        try:\n",
    "            if results.detections:\n",
    "                # Draw the face detection annotations on the image.\n",
    "                for detection in results.detections:\n",
    "                    \n",
    "                    # Get the bounding box of the face\n",
    "                    # and its characteristics\n",
    "                    bb = detection.location_data.relative_bounding_box\n",
    "                    x = bb.xmin\n",
    "                    y = bb.ymin\n",
    "                    h = bb.height\n",
    "                    w = bb.width\n",
    "\n",
    "                    camWidth = img.shape[1]\n",
    "                    camHeight = img.shape[0]\n",
    "\n",
    "                    # Convert x, y, h and w from percentages to pixels\n",
    "                    x = int(x * camWidth)\n",
    "                    y = int(y * camHeight)\n",
    "                    h = int(h * camHeight)\n",
    "                    w = int(w * camWidth)\n",
    "                    \n",
    "                    # Allows some time to move between the captures\n",
    "                    # Change the modulo value if you want more or less time between captures\n",
    "                    i = (i + 1) % 15\n",
    "                    if i == 0:\n",
    "                        file_name = \"face_\" + str(count) + \".jpg\"\n",
    "                        \n",
    "                        # Resize the image to 250x250px\n",
    "                        f = cv2.resize(img[y:y+h, x:x+w], (250, 250), interpolation = cv2.INTER_AREA)\n",
    "                        \n",
    "                        # Save the capture\n",
    "                        cv2.imwrite('./Output/faces/' + name + '/' + file_name, f)\n",
    "                        count += 1\n",
    "                        \n",
    "                        # Stop if we reached the wanted dataset size\n",
    "                        if count >= datasetSize:\n",
    "                            stop = True\n",
    "                    \n",
    "                    # Draw a rectangle to represent the bounding box\n",
    "                    cv2.rectangle(bounded_img, (x, y),(x + w, y + h), (255, 0, 0), 2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Increase output size for ergonomy\n",
    "        bounded_img = cv2.resize(bounded_img, (0,0), fx=1.5, fy=1.5)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', bounded_img)\n",
    "        \n",
    "        # Show the remaining frames to capture before the end\n",
    "        cv2.setWindowTitle('frame', 'frame : ' + str(abs(count - datasetSize)))\n",
    "        \n",
    "        # The 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading images from Masked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 444/444 [00:00<00:00, 801.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading images from Masked\n",
      "Begin loading images from Unmasked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 443/443 [00:00<00:00, 753.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading images from Unmasked\n",
      "Begin loading images from Half_Masked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 440/440 [00:00<00:00, 809.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading images from Half_Masked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the used directories (same as labels)\n",
    "label_names = [\"Masked\", \"Unmasked\", \"Half_Masked\"]\n",
    "\n",
    "# Use it if you want to limit the number of images per category\n",
    "# to avoid long loading times\n",
    "# Set it to 0 or less to have no limit\n",
    "limit = 0\n",
    "\n",
    "faces = []\n",
    "labels = []\n",
    "colors = []\n",
    "\n",
    "for id, d in enumerate(label_names):\n",
    "    print(\"Begin loading images from \" + d)\n",
    "    time.sleep(.5)\n",
    "    files = os.listdir('Output/faces/' + d)\n",
    "    \n",
    "    max_files = min(limit, len(files)) if limit > 0 and limit < len(files) else len(files)\n",
    "    # Load every image (in the defined limit),\n",
    "    # resize them to 250x250px\n",
    "    # and convert them to grayscales\n",
    "    with tqdm(total=max_files) as pbar:\n",
    "        for x,f in enumerate(files):\n",
    "            pbar.update(1)\n",
    "            if x >= max_files-1:\n",
    "                break\n",
    "            faces += [cv2.cvtColor(cv2.resize(cv2.imread('./Output/faces/'+ d + '/' + f), (250, 250), interpolation=cv2.INTER_AREA), cv2.COLOR_BGR2GRAY)]\n",
    "    #faces += [cv2.cvtColor(cv2.resize(cv2.imread('./Output/faces/'+ d + '/' + f), (250, 250), interpolation=cv2.INTER_AREA), cv2.COLOR_BGR2GRAY) for x,f in enumerate(files) if x < limit and limit > 0]\n",
    "    \n",
    "    # Fill the labels according to the index of the directory name\n",
    "    # Example : 0 for 'Masked', 1 for 'Unmasked' and 2 for 'Half_Masked'\n",
    "    labels += [id] * (len(faces)-len(labels))\n",
    "\n",
    "    # add random color in the list\n",
    "    c = [(np.random.randint(100,255), np.random.randint(100,255), np.random.randint(100,255))]\n",
    "    while c in colors:\n",
    "        c = [((np.random.randint(100,255), np.random.randint(100,255), np.random.randint(100,255)))]\n",
    "    colors += c\n",
    "    print(\"Finished loading images from \" + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1324/1324 [00:00<00:00, 29396.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443, 62500)\n",
      "(442, 62500)\n",
      "(439, 62500)\n",
      "(1324, 62500)\n",
      "(1324,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with_mask = []\n",
    "without_mask = []\n",
    "half_masked = []\n",
    "\n",
    "with tqdm(total=len(faces)) as pbar:\n",
    "    for f, l in zip(faces, labels):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # transform the image to a numpy array\n",
    "        data = np.array(f)\n",
    "        # reshape the image to a linear array\n",
    "        data = data.reshape(1, -1)\n",
    "        if l == 0:\n",
    "            with_mask.append(data)\n",
    "        elif l == 1:\n",
    "            without_mask.append(data)\n",
    "        elif l == 2:\n",
    "            half_masked.append(data)\n",
    "        else:\n",
    "            dumbass.append(data)\n",
    "\n",
    "with_mask = np.array(with_mask).reshape(len(with_mask), -1)\n",
    "without_mask = np.array(without_mask).reshape(len(without_mask), -1)\n",
    "half_masked = np.array(half_masked).reshape(len(half_masked), -1)\n",
    "\n",
    "print(with_mask.shape)\n",
    "print(without_mask.shape)\n",
    "print(half_masked.shape)\n",
    "\n",
    "# concatenate the arrays \n",
    "X = np.r_[with_mask, without_mask, half_masked]\n",
    "print(X.shape)\n",
    "\n",
    "labels = np.array(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(993, 62500)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, labels, test_size=0.25, random_state=42)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'svm_model.sav'\n",
    "\n",
    "# You can load the model if you want to train over the previous trainings\n",
    "#svm = joblib.load(filename)\n",
    "svm = SVC(kernel='rbf', C=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]0.8429003021148036\n"
     ]
    }
   ],
   "source": [
    "svm.fit(x_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.sav']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "#svm = joblib.load(filename)\n",
    "\n",
    "vid = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    while(True):\n",
    "        # Capture the video frame\n",
    "        # by frame\n",
    "        ret, img = vid.read()\n",
    "        cv2.flip(img, 1, img)\n",
    "        \n",
    "        img.flags.writeable = False\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = face_detection.process(img)\n",
    "\n",
    "        # Draw the face detection annotations on the image.\n",
    "        img.flags.writeable = True\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    \n",
    "                    # Get the bounding box of the face\n",
    "                    # and its characteristics\n",
    "                    bb = detection.location_data.relative_bounding_box\n",
    "                    x = bb.xmin\n",
    "                    y = bb.ymin\n",
    "                    h = bb.height\n",
    "                    w = bb.width\n",
    "\n",
    "                    camWidth = img.shape[1]\n",
    "                    camHeight = img.shape[0]\n",
    "\n",
    "                    # Convert x, y, h and w from percentages to pixels\n",
    "                    x = int(x * camWidth)\n",
    "                    y = int(y * camHeight)\n",
    "                    h = int(h * camHeight)\n",
    "                    w = int(w * camWidth)\n",
    "                    \n",
    "                    # Resize the face capture and grayscale\n",
    "                    toPredict = cv2.resize(img[y:y+h, x:x+w], (250, 250), interpolation = cv2.INTER_AREA)\n",
    "                    toPredict = cv2.cvtColor(toPredict, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    # Reshape the data\n",
    "                    toPredict = toPredict.reshape(1, -1)\n",
    "\n",
    "                    # Predict\n",
    "                    pred = svm.predict(toPredict)\n",
    "                    pred = int(pred[0])\n",
    "\n",
    "                    # if the predicted label is in the list of known labels, draw a rectangle around the face and label it\n",
    "                    if pred < len(label_names):\n",
    "                        cv2.rectangle(img,(x,y),(x+w,y+h),colors[pred],2)\n",
    "                        cv2.putText(img, label_names[pred], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, colors[pred], 2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        img = cv2.resize(img, (0,0), fx=1.5, fy=1.5)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', img)\n",
    "        \n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8457d5f13b68ea6baddc1bbfa45220c29b0ac974c0b776b8f1dc3cf598baeca6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
